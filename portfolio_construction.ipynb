{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F7Ymrqispw1q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tVXX07BqUfW"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('macro_index_returns.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FRoqdYRqjoH"
      },
      "outputs": [],
      "source": [
        "columns = [\"RETX\", \"date\",\"TICKER\"]\n",
        "df = df[columns]\n",
        "\n",
        "def convert_to_number(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df['ret_parsed'] = df['RETX'].apply(convert_to_number)\n",
        "\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df.set_index(\"date\", inplace=True)\n",
        "df.drop(columns=[\"RETX\"], inplace=True)\n",
        "\n",
        "df = df.pivot_table(index=df.index, columns=\"TICKER\", values=\"ret_parsed\", aggfunc=\"first\")\n",
        "\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTxavHnLqoDK"
      },
      "outputs": [],
      "source": [
        "# 2. Compute stable rolling z\n",
        "def safe_rolling_zscore(df, window):\n",
        "    rolling_mean = df.rolling(window).mean()\n",
        "    rolling_std = df.rolling(window).std()\n",
        "\n",
        "    # If std == 0 â†’ return 0 instead of NaN or inf\n",
        "    z = (df - rolling_mean) / rolling_std.replace(0, np.nan)\n",
        "    z = z.fillna(0)\n",
        "\n",
        "    return z\n",
        "\n",
        "returns_df = safe_rolling_zscore(df, window=60)\n",
        "\n",
        "# 3. Replace infinities / residual NaN\n",
        "returns_df = returns_df.replace([np.inf, -np.inf], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOKLwO-xq4GS"
      },
      "outputs": [],
      "source": [
        "rolling_vol_df  =df.rolling(60).std() * np.sqrt(252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGTRC_GGtlxK"
      },
      "outputs": [],
      "source": [
        "rolling_vol_df = rolling_vol_df[60:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG8yLjKOuLCG",
        "outputId": "9c75ea57-1a61-446f-a081-0aa6c65c4ebd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4160"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(rolling_vol_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "n_dnajnfuzAL",
        "outputId": "b9a384ec-82c4-41d1-a47e-28f4714cd29d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"rolling_vol_df\",\n  \"rows\": 4160,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2008-06-23 00:00:00\",\n        \"max\": \"2024-12-31 00:00:00\",\n        \"num_unique_values\": 4160,\n        \"samples\": [\n          \"2022-07-14 00:00:00\",\n          \"2023-03-09 00:00:00\",\n          \"2009-03-04 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GLD\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06376159876661665,\n        \"min\": 0.07517625956039255,\n        \"max\": 0.5152090044706853,\n        \"num_unique_values\": 4160,\n        \"samples\": [\n          0.14388766432661784,\n          0.14377189503479063,\n          0.28558499980049873\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IEF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.023282914554500288,\n        \"min\": 0.027948118354175558,\n        \"max\": 0.13203854231963835,\n        \"num_unique_values\": 4159,\n        \"samples\": [\n          0.10671356105316455,\n          0.09027635926882482,\n          0.1047429122801092\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SPY\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10808692383482212,\n        \"min\": 0.05064637353788067,\n        \"max\": 0.758344006264801,\n        \"num_unique_values\": 4160,\n        \"samples\": [\n          0.2884614111240134,\n          0.1747335706219355,\n          0.35971936750501254\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"USO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16231502133149842,\n        \"min\": 0.10466438704767014,\n        \"max\": 1.3118723036696789,\n        \"num_unique_values\": 4159,\n        \"samples\": [\n          0.43461042545046463,\n          0.3069052186075352,\n          0.7612283743948753\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UUP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02846977403565308,\n        \"min\": 0.030290561257173478,\n        \"max\": 0.18681896536387976,\n        \"num_unique_values\": 4149,\n        \"samples\": [\n          0.1791870095143855,\n          0.06899629877235985,\n          0.05460708055523661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "rolling_vol_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1b9624c0-cb47-4cb8-bd97-7da02ddd499e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>TICKER</th>\n",
              "      <th>GLD</th>\n",
              "      <th>IEF</th>\n",
              "      <th>SPY</th>\n",
              "      <th>USO</th>\n",
              "      <th>UUP</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-06-23</th>\n",
              "      <td>0.244043</td>\n",
              "      <td>0.074710</td>\n",
              "      <td>0.173774</td>\n",
              "      <td>0.356072</td>\n",
              "      <td>0.100404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-24</th>\n",
              "      <td>0.242178</td>\n",
              "      <td>0.075606</td>\n",
              "      <td>0.173672</td>\n",
              "      <td>0.346093</td>\n",
              "      <td>0.100544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-25</th>\n",
              "      <td>0.228262</td>\n",
              "      <td>0.071889</td>\n",
              "      <td>0.157800</td>\n",
              "      <td>0.350188</td>\n",
              "      <td>0.095844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-26</th>\n",
              "      <td>0.233413</td>\n",
              "      <td>0.072809</td>\n",
              "      <td>0.166927</td>\n",
              "      <td>0.353790</td>\n",
              "      <td>0.094608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-06-27</th>\n",
              "      <td>0.234150</td>\n",
              "      <td>0.073648</td>\n",
              "      <td>0.167010</td>\n",
              "      <td>0.353505</td>\n",
              "      <td>0.094526</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b9624c0-cb47-4cb8-bd97-7da02ddd499e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b9624c0-cb47-4cb8-bd97-7da02ddd499e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b9624c0-cb47-4cb8-bd97-7da02ddd499e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5e39d6f7-aafd-4889-97ea-6dec5981bf25\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e39d6f7-aafd-4889-97ea-6dec5981bf25')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5e39d6f7-aafd-4889-97ea-6dec5981bf25 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "TICKER           GLD       IEF       SPY       USO       UUP\n",
              "date                                                        \n",
              "2008-06-23  0.244043  0.074710  0.173774  0.356072  0.100404\n",
              "2008-06-24  0.242178  0.075606  0.173672  0.346093  0.100544\n",
              "2008-06-25  0.228262  0.071889  0.157800  0.350188  0.095844\n",
              "2008-06-26  0.233413  0.072809  0.166927  0.353790  0.094608\n",
              "2008-06-27  0.234150  0.073648  0.167010  0.353505  0.094526"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rolling_vol_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC4BpAyuuYam",
        "outputId": "ebce3a4b-7622-4541-d629-05a6c7c0ee62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4279"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(returns_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWa74CunvGUq"
      },
      "outputs": [],
      "source": [
        "returns_df, rolling_vol_df = returns_df.align(rolling_vol_df, join='inner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo-iKU6a1tAO"
      },
      "outputs": [],
      "source": [
        "df, rolling_vol_df = df.align(rolling_vol_df, join='inner')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "344u2paLsUrC"
      },
      "source": [
        "training section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krktjk9AsAuz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Suppose returns_df and rolling_vol_df have the same index\n",
        "T = len(returns_df)\n",
        "\n",
        "returns_df = df\n",
        "\n",
        "train_ratio = 0.8\n",
        "split_idx = int(T * train_ratio)\n",
        "\n",
        "returns_train = returns_df.iloc[:split_idx]\n",
        "returns_test  = returns_df.iloc[split_idx:]\n",
        "\n",
        "vol_train = rolling_vol_df.iloc[:split_idx]\n",
        "vol_test  = rolling_vol_df.iloc[split_idx:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeDzuY7SrB3A"
      },
      "outputs": [],
      "source": [
        "window = 20  # past window size (days)\n",
        "\n",
        "# ----- TRAIN SET -----\n",
        "X_train, vol_train_list, y_train = [], [], []\n",
        "\n",
        "for t in range(window, len(returns_train) - 1):\n",
        "    past = returns_train.iloc[t-window:t].values.flatten()\n",
        "    vol_t = vol_train.iloc[t].values\n",
        "    y_t1  = returns_train.iloc[t+1].values\n",
        "\n",
        "    X_train.append(past)\n",
        "    vol_train_list.append(vol_t)\n",
        "    y_train.append(y_t1)\n",
        "\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "vol_train_list = np.array(vol_train_list, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "\n",
        "# ----- TEST SET -----\n",
        "X_test, vol_test_list, y_test = [], [], []\n",
        "\n",
        "for t in range(window, len(returns_test) - 1):\n",
        "    past = returns_test.iloc[t-window:t].values.flatten()\n",
        "    vol_t = vol_test.iloc[t].values\n",
        "    y_t1  = returns_test.iloc[t+1].values\n",
        "\n",
        "    X_test.append(past)\n",
        "    vol_test_list.append(vol_t)\n",
        "    y_test.append(y_t1)\n",
        "\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "vol_test_list = np.array(vol_test_list, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAUIRnumsK4S"
      },
      "outputs": [],
      "source": [
        "class MacroPortfolioDataset(Dataset):\n",
        "    def __init__(self, X, vol, y):\n",
        "        self.X = torch.from_numpy(X)      # (N, D)\n",
        "        self.vol = torch.from_numpy(vol)  # (N, 5)\n",
        "        self.y = torch.from_numpy(y)      # (N, 5)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.vol[idx], self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Saw-_3obrJGW"
      },
      "outputs": [],
      "source": [
        "train_dataset = MacroPortfolioDataset(X_train, vol_train_list, y_train)\n",
        "test_dataset  = MacroPortfolioDataset(X_test,  vol_test_list,  y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=256, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeuLuOKorQNh"
      },
      "outputs": [],
      "source": [
        "class MacroPortfolioNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 6)  # 5 risky + 1 cash\n",
        "        )\n",
        "\n",
        "    def forward(self, x, vol):\n",
        "        \"\"\"\n",
        "        x: (batch, D)        - features\n",
        "        vol: (batch, 5)      - rolling vol for each asset (positive)\n",
        "        Returns:\n",
        "            weights: (batch, 6) - 5 risky + 1 cash\n",
        "        \"\"\"\n",
        "        logits = self.net(x)          # (batch, 6)\n",
        "        risk_scores = logits[:, :5]   # (batch, 5)\n",
        "        cash_logit = logits[:, 5]     # (batch,)\n",
        "\n",
        "        # ---- Cash weight (long-only, in [0,1]) ----\n",
        "        cash_weight = torch.sigmoid(cash_logit)  # (batch,)\n",
        "        risk_budget = 1.0 - cash_weight         # fraction of capital in risky assets\n",
        "\n",
        "        # ---- Vol-normalized risky directions ----\n",
        "        eps = 1e-8\n",
        "        vol_adj = risk_scores / (vol + eps)     # (batch, 5)\n",
        "\n",
        "        # Normalize so sum |weights| = 1 for the risky component\n",
        "        abs_sum = torch.sum(torch.abs(vol_adj), dim=1, keepdim=True) + eps\n",
        "        risk_dir = vol_adj / abs_sum            # (batch, 5), sum |risk_dir| = 1\n",
        "\n",
        "        # Scale by risk budget\n",
        "        risky_weights = risk_dir * risk_budget.unsqueeze(1)  # (batch, 5)\n",
        "\n",
        "        # Stack with cash\n",
        "        cash_weight = cash_weight.unsqueeze(1)  # (batch, 1)\n",
        "        weights = torch.cat([risky_weights, cash_weight], dim=1)  # (batch, 6)\n",
        "        return weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkNFwq_OrRkh"
      },
      "outputs": [],
      "source": [
        "def sharpe_ratio_loss(portfolio_returns, eps=1e-6):\n",
        "    \"\"\"\n",
        "    portfolio_returns: (batch,) tensor of daily portfolio returns\n",
        "    Returns: loss = -Sharpe\n",
        "    \"\"\"\n",
        "    mean = portfolio_returns.mean()\n",
        "    std = portfolio_returns.std(unbiased=False) + eps\n",
        "    sharpe = mean / std\n",
        "    return -sharpe\n",
        "\n",
        "\n",
        "def mean_variance_loss(port_ret, risky_w, lambda_risk=1.0, lambda_lev=1e-3, eps=1e-8):\n",
        "    \"\"\"\n",
        "    port_ret: (batch,) portfolio returns\n",
        "    risky_w:  (batch, 5) risky weights only\n",
        "    \"\"\"\n",
        "    mean_ret = port_ret.mean()\n",
        "    var_ret  = port_ret.var(unbiased=False)\n",
        "\n",
        "    # leverage penalty (squared L2 of risky weights)\n",
        "    lev_penalty = (risky_w.pow(2).sum(dim=1)).mean()\n",
        "\n",
        "    loss = -mean_ret + lambda_risk * var_ret + lambda_lev * lev_penalty\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEY_gnzzrVst",
        "outputId": "2fba102f-1845-458d-fd94-217c88e464aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: -0.0744\n",
            "Epoch 2, Train Loss: -0.0810\n",
            "Epoch 3, Train Loss: -0.0824\n",
            "Epoch 4, Train Loss: -0.0771\n",
            "Epoch 5, Train Loss: -0.0814\n",
            "Epoch 6, Train Loss: -0.0856\n",
            "Epoch 7, Train Loss: -0.0866\n",
            "Epoch 8, Train Loss: -0.0844\n",
            "Epoch 9, Train Loss: -0.0891\n",
            "Epoch 10, Train Loss: -0.0903\n",
            "Epoch 11, Train Loss: -0.0906\n",
            "Epoch 12, Train Loss: -0.0867\n",
            "Epoch 13, Train Loss: -0.0861\n",
            "Epoch 14, Train Loss: -0.0914\n",
            "Epoch 15, Train Loss: -0.0909\n",
            "Epoch 16, Train Loss: -0.0986\n",
            "Epoch 17, Train Loss: -0.0966\n",
            "Epoch 18, Train Loss: -0.0964\n",
            "Epoch 19, Train Loss: -0.0995\n",
            "Epoch 20, Train Loss: -0.0927\n",
            "Epoch 21, Train Loss: -0.1035\n",
            "Epoch 22, Train Loss: -0.1004\n",
            "Epoch 23, Train Loss: -0.1022\n",
            "Epoch 24, Train Loss: -0.1099\n",
            "Epoch 25, Train Loss: -0.1071\n",
            "Epoch 26, Train Loss: -0.1119\n",
            "Epoch 27, Train Loss: -0.1139\n",
            "Epoch 28, Train Loss: -0.1226\n",
            "Epoch 29, Train Loss: -0.1136\n",
            "Epoch 30, Train Loss: -0.1179\n",
            "Epoch 31, Train Loss: -0.1215\n",
            "Epoch 32, Train Loss: -0.1193\n",
            "Epoch 33, Train Loss: -0.1201\n",
            "Epoch 34, Train Loss: -0.1232\n",
            "Epoch 35, Train Loss: -0.1215\n",
            "Epoch 36, Train Loss: -0.1257\n",
            "Epoch 37, Train Loss: -0.1273\n",
            "Epoch 38, Train Loss: -0.1396\n",
            "Epoch 39, Train Loss: -0.1326\n",
            "Epoch 40, Train Loss: -0.1426\n",
            "Epoch 41, Train Loss: -0.1389\n",
            "Epoch 42, Train Loss: -0.1427\n",
            "Epoch 43, Train Loss: -0.1495\n",
            "Epoch 44, Train Loss: -0.1516\n",
            "Epoch 45, Train Loss: -0.1620\n",
            "Epoch 46, Train Loss: -0.1522\n",
            "Epoch 47, Train Loss: -0.1590\n",
            "Epoch 48, Train Loss: -0.1605\n",
            "Epoch 49, Train Loss: -0.1570\n",
            "Epoch 50, Train Loss: -0.1596\n",
            "Epoch 51, Train Loss: -0.1643\n",
            "Epoch 52, Train Loss: -0.1604\n",
            "Epoch 53, Train Loss: -0.1667\n",
            "Epoch 54, Train Loss: -0.1670\n",
            "Epoch 55, Train Loss: -0.1698\n",
            "Epoch 56, Train Loss: -0.1635\n",
            "Epoch 57, Train Loss: -0.1693\n",
            "Epoch 58, Train Loss: -0.1712\n",
            "Epoch 59, Train Loss: -0.1738\n",
            "Epoch 60, Train Loss: -0.1633\n",
            "Epoch 61, Train Loss: -0.1750\n",
            "Epoch 62, Train Loss: -0.1756\n",
            "Epoch 63, Train Loss: -0.1784\n",
            "Epoch 64, Train Loss: -0.1844\n",
            "Epoch 65, Train Loss: -0.1902\n",
            "Epoch 66, Train Loss: -0.1816\n",
            "Epoch 67, Train Loss: -0.1860\n",
            "Epoch 68, Train Loss: -0.1812\n",
            "Epoch 69, Train Loss: -0.1798\n",
            "Epoch 70, Train Loss: -0.1811\n",
            "Epoch 71, Train Loss: -0.1821\n",
            "Epoch 72, Train Loss: -0.1978\n",
            "Epoch 73, Train Loss: -0.1907\n",
            "Epoch 74, Train Loss: -0.1871\n",
            "Epoch 75, Train Loss: -0.1946\n",
            "Epoch 76, Train Loss: -0.1921\n",
            "Epoch 77, Train Loss: -0.2041\n",
            "Epoch 78, Train Loss: -0.1964\n",
            "Epoch 79, Train Loss: -0.1976\n",
            "Epoch 80, Train Loss: -0.2050\n",
            "Epoch 81, Train Loss: -0.2080\n",
            "Epoch 82, Train Loss: -0.2071\n",
            "Epoch 83, Train Loss: -0.2033\n",
            "Epoch 84, Train Loss: -0.2067\n",
            "Epoch 85, Train Loss: -0.2075\n",
            "Epoch 86, Train Loss: -0.2049\n",
            "Epoch 87, Train Loss: -0.2114\n",
            "Epoch 88, Train Loss: -0.2041\n",
            "Epoch 89, Train Loss: -0.2127\n",
            "Epoch 90, Train Loss: -0.2111\n",
            "Epoch 91, Train Loss: -0.2158\n",
            "Epoch 92, Train Loss: -0.2160\n",
            "Epoch 93, Train Loss: -0.2184\n",
            "Epoch 94, Train Loss: -0.2205\n",
            "Epoch 95, Train Loss: -0.2182\n",
            "Epoch 96, Train Loss: -0.2174\n",
            "Epoch 97, Train Loss: -0.2197\n",
            "Epoch 98, Train Loss: -0.2252\n",
            "Epoch 99, Train Loss: -0.2222\n",
            "Epoch 100, Train Loss: -0.2232\n",
            "Epoch 101, Train Loss: -0.2291\n",
            "Epoch 102, Train Loss: -0.2320\n",
            "Epoch 103, Train Loss: -0.2236\n",
            "Epoch 104, Train Loss: -0.2329\n",
            "Epoch 105, Train Loss: -0.2236\n",
            "Epoch 106, Train Loss: -0.2352\n",
            "Epoch 107, Train Loss: -0.2323\n",
            "Epoch 108, Train Loss: -0.2305\n",
            "Epoch 109, Train Loss: -0.2311\n",
            "Epoch 110, Train Loss: -0.2426\n",
            "Epoch 111, Train Loss: -0.2266\n",
            "Epoch 112, Train Loss: -0.2322\n",
            "Epoch 113, Train Loss: -0.2368\n",
            "Epoch 114, Train Loss: -0.2398\n",
            "Epoch 115, Train Loss: -0.2457\n",
            "Epoch 116, Train Loss: -0.2360\n",
            "Epoch 117, Train Loss: -0.2406\n",
            "Epoch 118, Train Loss: -0.2352\n",
            "Epoch 119, Train Loss: -0.2496\n",
            "Epoch 120, Train Loss: -0.2449\n",
            "Epoch 121, Train Loss: -0.2421\n",
            "Epoch 122, Train Loss: -0.2514\n",
            "Epoch 123, Train Loss: -0.2401\n",
            "Epoch 124, Train Loss: -0.2479\n",
            "Epoch 125, Train Loss: -0.2407\n",
            "Epoch 126, Train Loss: -0.2449\n",
            "Epoch 127, Train Loss: -0.2483\n",
            "Epoch 128, Train Loss: -0.2296\n",
            "Epoch 129, Train Loss: -0.2459\n",
            "Epoch 130, Train Loss: -0.2482\n",
            "Epoch 131, Train Loss: -0.2593\n",
            "Epoch 132, Train Loss: -0.2413\n",
            "Epoch 133, Train Loss: -0.2437\n",
            "Epoch 134, Train Loss: -0.2511\n",
            "Epoch 135, Train Loss: -0.2474\n",
            "Epoch 136, Train Loss: -0.2604\n",
            "Epoch 137, Train Loss: -0.2603\n",
            "Epoch 138, Train Loss: -0.2511\n",
            "Epoch 139, Train Loss: -0.2569\n",
            "Epoch 140, Train Loss: -0.2554\n",
            "Epoch 141, Train Loss: -0.2535\n",
            "Epoch 142, Train Loss: -0.2546\n",
            "Epoch 143, Train Loss: -0.2589\n",
            "Epoch 144, Train Loss: -0.2644\n",
            "Epoch 145, Train Loss: -0.2468\n",
            "Epoch 146, Train Loss: -0.2622\n",
            "Epoch 147, Train Loss: -0.2596\n",
            "Epoch 148, Train Loss: -0.2543\n",
            "Epoch 149, Train Loss: -0.2578\n",
            "Epoch 150, Train Loss: -0.2605\n",
            "Epoch 151, Train Loss: -0.2666\n",
            "Epoch 152, Train Loss: -0.2646\n",
            "Epoch 153, Train Loss: -0.2684\n",
            "Epoch 154, Train Loss: -0.2618\n",
            "Epoch 155, Train Loss: -0.2691\n",
            "Epoch 156, Train Loss: -0.2718\n",
            "Epoch 157, Train Loss: -0.2652\n",
            "Epoch 158, Train Loss: -0.2741\n",
            "Epoch 159, Train Loss: -0.2718\n",
            "Epoch 160, Train Loss: -0.2687\n",
            "Epoch 161, Train Loss: -0.2682\n",
            "Epoch 162, Train Loss: -0.2647\n",
            "Epoch 163, Train Loss: -0.2625\n",
            "Epoch 164, Train Loss: -0.2736\n",
            "Epoch 165, Train Loss: -0.2777\n",
            "Epoch 166, Train Loss: -0.2745\n",
            "Epoch 167, Train Loss: -0.2794\n",
            "Epoch 168, Train Loss: -0.2681\n",
            "Epoch 169, Train Loss: -0.2714\n",
            "Epoch 170, Train Loss: -0.2805\n",
            "Epoch 171, Train Loss: -0.2795\n",
            "Epoch 172, Train Loss: -0.2845\n",
            "Epoch 173, Train Loss: -0.2626\n",
            "Epoch 174, Train Loss: -0.2785\n",
            "Epoch 175, Train Loss: -0.2787\n",
            "Epoch 176, Train Loss: -0.2774\n",
            "Epoch 177, Train Loss: -0.2721\n",
            "Epoch 178, Train Loss: -0.2644\n",
            "Epoch 179, Train Loss: -0.2576\n",
            "Epoch 180, Train Loss: -0.2746\n",
            "Epoch 181, Train Loss: -0.2805\n",
            "Epoch 182, Train Loss: -0.2818\n",
            "Epoch 183, Train Loss: -0.2768\n",
            "Epoch 184, Train Loss: -0.2761\n",
            "Epoch 185, Train Loss: -0.2795\n",
            "Epoch 186, Train Loss: -0.2754\n",
            "Epoch 187, Train Loss: -0.2746\n",
            "Epoch 188, Train Loss: -0.2845\n",
            "Epoch 189, Train Loss: -0.2795\n",
            "Epoch 190, Train Loss: -0.2827\n",
            "Epoch 191, Train Loss: -0.2883\n",
            "Epoch 192, Train Loss: -0.2746\n",
            "Epoch 193, Train Loss: -0.2819\n",
            "Epoch 194, Train Loss: -0.2826\n",
            "Epoch 195, Train Loss: -0.2785\n",
            "Epoch 196, Train Loss: -0.2876\n",
            "Epoch 197, Train Loss: -0.2764\n",
            "Epoch 198, Train Loss: -0.2800\n",
            "Epoch 199, Train Loss: -0.2808\n",
            "Epoch 200, Train Loss: -0.2805\n",
            "Epoch 201, Train Loss: -0.2771\n",
            "Epoch 202, Train Loss: -0.2814\n",
            "Epoch 203, Train Loss: -0.2764\n",
            "Epoch 204, Train Loss: -0.2736\n",
            "Epoch 205, Train Loss: -0.2823\n",
            "Epoch 206, Train Loss: -0.2855\n",
            "Epoch 207, Train Loss: -0.2832\n",
            "Epoch 208, Train Loss: -0.2898\n",
            "Epoch 209, Train Loss: -0.2757\n",
            "Epoch 210, Train Loss: -0.2829\n",
            "Epoch 211, Train Loss: -0.2874\n",
            "Epoch 212, Train Loss: -0.2811\n",
            "Epoch 213, Train Loss: -0.2862\n",
            "Epoch 214, Train Loss: -0.2892\n",
            "Epoch 215, Train Loss: -0.2839\n",
            "Epoch 216, Train Loss: -0.2879\n",
            "Epoch 217, Train Loss: -0.2746\n",
            "Epoch 218, Train Loss: -0.2739\n",
            "Epoch 219, Train Loss: -0.2833\n",
            "Epoch 220, Train Loss: -0.2768\n",
            "Epoch 221, Train Loss: -0.2794\n",
            "Epoch 222, Train Loss: -0.2848\n",
            "Epoch 223, Train Loss: -0.2836\n",
            "Epoch 224, Train Loss: -0.2825\n",
            "Epoch 225, Train Loss: -0.2787\n",
            "Epoch 226, Train Loss: -0.2855\n",
            "Epoch 227, Train Loss: -0.2842\n",
            "Epoch 228, Train Loss: -0.2886\n",
            "Epoch 229, Train Loss: -0.2919\n",
            "Epoch 230, Train Loss: -0.2874\n",
            "Epoch 231, Train Loss: -0.2857\n",
            "Epoch 232, Train Loss: -0.2895\n",
            "Epoch 233, Train Loss: -0.2847\n",
            "Epoch 234, Train Loss: -0.2902\n",
            "Epoch 235, Train Loss: -0.2908\n",
            "Epoch 236, Train Loss: -0.2693\n",
            "Epoch 237, Train Loss: -0.2547\n",
            "Epoch 238, Train Loss: -0.2781\n",
            "Epoch 239, Train Loss: -0.2810\n",
            "Epoch 240, Train Loss: -0.2819\n",
            "Epoch 241, Train Loss: -0.2946\n",
            "Epoch 242, Train Loss: -0.2907\n",
            "Epoch 243, Train Loss: -0.2940\n",
            "Epoch 244, Train Loss: -0.2874\n",
            "Epoch 245, Train Loss: -0.2842\n",
            "Epoch 246, Train Loss: -0.2943\n",
            "Epoch 247, Train Loss: -0.2892\n",
            "Epoch 248, Train Loss: -0.2882\n",
            "Epoch 249, Train Loss: -0.2854\n",
            "Epoch 250, Train Loss: -0.2907\n",
            "Epoch 251, Train Loss: -0.2906\n",
            "Epoch 252, Train Loss: -0.2883\n",
            "Epoch 253, Train Loss: -0.2942\n",
            "Epoch 254, Train Loss: -0.2955\n",
            "Epoch 255, Train Loss: -0.2888\n",
            "Epoch 256, Train Loss: -0.2795\n",
            "Epoch 257, Train Loss: -0.2879\n",
            "Epoch 258, Train Loss: -0.2938\n",
            "Epoch 259, Train Loss: -0.2843\n",
            "Epoch 260, Train Loss: -0.2947\n",
            "Epoch 261, Train Loss: -0.2985\n",
            "Epoch 262, Train Loss: -0.2872\n",
            "Epoch 263, Train Loss: -0.2898\n",
            "Epoch 264, Train Loss: -0.2861\n",
            "Epoch 265, Train Loss: -0.2828\n",
            "Epoch 266, Train Loss: -0.2905\n",
            "Epoch 267, Train Loss: -0.2879\n",
            "Epoch 268, Train Loss: -0.2903\n",
            "Epoch 269, Train Loss: -0.2873\n",
            "Epoch 270, Train Loss: -0.2884\n",
            "Epoch 271, Train Loss: -0.2922\n",
            "Epoch 272, Train Loss: -0.2915\n",
            "Epoch 273, Train Loss: -0.2933\n",
            "Epoch 274, Train Loss: -0.2939\n",
            "Epoch 275, Train Loss: -0.2976\n",
            "Epoch 276, Train Loss: -0.2738\n",
            "Epoch 277, Train Loss: -0.2818\n",
            "Epoch 278, Train Loss: -0.2917\n",
            "Epoch 279, Train Loss: -0.2893\n",
            "Epoch 280, Train Loss: -0.2909\n",
            "Epoch 281, Train Loss: -0.2911\n",
            "Epoch 282, Train Loss: -0.2867\n",
            "Epoch 283, Train Loss: -0.2859\n",
            "Epoch 284, Train Loss: -0.2995\n",
            "Epoch 285, Train Loss: -0.2831\n",
            "Epoch 286, Train Loss: -0.2875\n",
            "Epoch 287, Train Loss: -0.2992\n",
            "Epoch 288, Train Loss: -0.2871\n",
            "Epoch 289, Train Loss: -0.2888\n",
            "Epoch 290, Train Loss: -0.2961\n",
            "Epoch 291, Train Loss: -0.2999\n",
            "Epoch 292, Train Loss: -0.2889\n",
            "Epoch 293, Train Loss: -0.2998\n",
            "Epoch 294, Train Loss: -0.2886\n",
            "Epoch 295, Train Loss: -0.2936\n",
            "Epoch 296, Train Loss: -0.2944\n",
            "Epoch 297, Train Loss: -0.2996\n",
            "Epoch 298, Train Loss: -0.2983\n",
            "Epoch 299, Train Loss: -0.2866\n",
            "Epoch 300, Train Loss: -0.2897\n",
            "Epoch 301, Train Loss: -0.2897\n",
            "Epoch 302, Train Loss: -0.2972\n",
            "Epoch 303, Train Loss: -0.2930\n",
            "Epoch 304, Train Loss: -0.2832\n",
            "Epoch 305, Train Loss: -0.2879\n",
            "Epoch 306, Train Loss: -0.2919\n",
            "Epoch 307, Train Loss: -0.2955\n",
            "Epoch 308, Train Loss: -0.2956\n",
            "Epoch 309, Train Loss: -0.2935\n",
            "Epoch 310, Train Loss: -0.2989\n",
            "Epoch 311, Train Loss: -0.3019\n",
            "Epoch 312, Train Loss: -0.2882\n",
            "Epoch 313, Train Loss: -0.2986\n",
            "Epoch 314, Train Loss: -0.2946\n",
            "Epoch 315, Train Loss: -0.2986\n",
            "Epoch 316, Train Loss: -0.2953\n",
            "Epoch 317, Train Loss: -0.2948\n",
            "Epoch 318, Train Loss: -0.2959\n",
            "Epoch 319, Train Loss: -0.2926\n",
            "Epoch 320, Train Loss: -0.2917\n",
            "Epoch 321, Train Loss: -0.2980\n",
            "Epoch 322, Train Loss: -0.2998\n",
            "Epoch 323, Train Loss: -0.2864\n",
            "Epoch 324, Train Loss: -0.2924\n",
            "Epoch 325, Train Loss: -0.3076\n",
            "Epoch 326, Train Loss: -0.3018\n",
            "Epoch 327, Train Loss: -0.3046\n",
            "Epoch 328, Train Loss: -0.2971\n",
            "Epoch 329, Train Loss: -0.2994\n",
            "Epoch 330, Train Loss: -0.2971\n",
            "Epoch 331, Train Loss: -0.2896\n",
            "Epoch 332, Train Loss: -0.2946\n",
            "Epoch 333, Train Loss: -0.2979\n",
            "Epoch 334, Train Loss: -0.2975\n",
            "Epoch 335, Train Loss: -0.2869\n",
            "Epoch 336, Train Loss: -0.2968\n",
            "Epoch 337, Train Loss: -0.2939\n",
            "Epoch 338, Train Loss: -0.2902\n",
            "Epoch 339, Train Loss: -0.2771\n",
            "Epoch 340, Train Loss: -0.2946\n",
            "Epoch 341, Train Loss: -0.2938\n",
            "Epoch 342, Train Loss: -0.2991\n",
            "Epoch 343, Train Loss: -0.3064\n",
            "Epoch 344, Train Loss: -0.3022\n",
            "Epoch 345, Train Loss: -0.2919\n",
            "Epoch 346, Train Loss: -0.2889\n",
            "Epoch 347, Train Loss: -0.2998\n",
            "Epoch 348, Train Loss: -0.2907\n",
            "Epoch 349, Train Loss: -0.3013\n",
            "Epoch 350, Train Loss: -0.3037\n",
            "Epoch 351, Train Loss: -0.2792\n",
            "Epoch 352, Train Loss: -0.3034\n",
            "Epoch 353, Train Loss: -0.2985\n",
            "Epoch 354, Train Loss: -0.3011\n",
            "Epoch 355, Train Loss: -0.2874\n",
            "Epoch 356, Train Loss: -0.2973\n",
            "Epoch 357, Train Loss: -0.3016\n",
            "Epoch 358, Train Loss: -0.2983\n",
            "Epoch 359, Train Loss: -0.2973\n",
            "Epoch 360, Train Loss: -0.2996\n",
            "Epoch 361, Train Loss: -0.2944\n",
            "Epoch 362, Train Loss: -0.3056\n",
            "Epoch 363, Train Loss: -0.3052\n",
            "Epoch 364, Train Loss: -0.2989\n",
            "Epoch 365, Train Loss: -0.3031\n",
            "Epoch 366, Train Loss: -0.2980\n",
            "Epoch 367, Train Loss: -0.3019\n",
            "Epoch 368, Train Loss: -0.2978\n",
            "Epoch 369, Train Loss: -0.2959\n",
            "Epoch 370, Train Loss: -0.2979\n",
            "Epoch 371, Train Loss: -0.3074\n",
            "Epoch 372, Train Loss: -0.2883\n",
            "Epoch 373, Train Loss: -0.2943\n",
            "Epoch 374, Train Loss: -0.3129\n",
            "Epoch 375, Train Loss: -0.3096\n",
            "Epoch 376, Train Loss: -0.3127\n",
            "Epoch 377, Train Loss: -0.3057\n",
            "Epoch 378, Train Loss: -0.3116\n",
            "Epoch 379, Train Loss: -0.3045\n",
            "Epoch 380, Train Loss: -0.3030\n",
            "Epoch 381, Train Loss: -0.3123\n",
            "Epoch 382, Train Loss: -0.3149\n",
            "Epoch 383, Train Loss: -0.3052\n",
            "Epoch 384, Train Loss: -0.3102\n",
            "Epoch 385, Train Loss: -0.3035\n",
            "Epoch 386, Train Loss: -0.3025\n",
            "Epoch 387, Train Loss: -0.3022\n",
            "Epoch 388, Train Loss: -0.3086\n",
            "Epoch 389, Train Loss: -0.2933\n",
            "Epoch 390, Train Loss: -0.3069\n",
            "Epoch 391, Train Loss: -0.2983\n",
            "Epoch 392, Train Loss: -0.3058\n",
            "Epoch 393, Train Loss: -0.2972\n",
            "Epoch 394, Train Loss: -0.3016\n",
            "Epoch 395, Train Loss: -0.3071\n",
            "Epoch 396, Train Loss: -0.3096\n",
            "Epoch 397, Train Loss: -0.3113\n",
            "Epoch 398, Train Loss: -0.3060\n",
            "Epoch 399, Train Loss: -0.3021\n",
            "Epoch 400, Train Loss: -0.3136\n",
            "Epoch 401, Train Loss: -0.3150\n",
            "Epoch 402, Train Loss: -0.3101\n",
            "Epoch 403, Train Loss: -0.3035\n",
            "Epoch 404, Train Loss: -0.2977\n",
            "Epoch 405, Train Loss: -0.3132\n",
            "Epoch 406, Train Loss: -0.3119\n",
            "Epoch 407, Train Loss: -0.3095\n",
            "Epoch 408, Train Loss: -0.3062\n",
            "Epoch 409, Train Loss: -0.3056\n",
            "Epoch 410, Train Loss: -0.3072\n",
            "Epoch 411, Train Loss: -0.3062\n",
            "Epoch 412, Train Loss: -0.2867\n",
            "Epoch 413, Train Loss: -0.2871\n",
            "Epoch 414, Train Loss: -0.2914\n",
            "Epoch 415, Train Loss: -0.3021\n",
            "Epoch 416, Train Loss: -0.2885\n",
            "Epoch 417, Train Loss: -0.3123\n",
            "Epoch 418, Train Loss: -0.2944\n",
            "Epoch 419, Train Loss: -0.3135\n",
            "Epoch 420, Train Loss: -0.3175\n",
            "Epoch 421, Train Loss: -0.3116\n",
            "Epoch 422, Train Loss: -0.3094\n",
            "Epoch 423, Train Loss: -0.3216\n",
            "Epoch 424, Train Loss: -0.3149\n",
            "Epoch 425, Train Loss: -0.3044\n",
            "Epoch 426, Train Loss: -0.3137\n",
            "Epoch 427, Train Loss: -0.3179\n",
            "Epoch 428, Train Loss: -0.3124\n",
            "Epoch 429, Train Loss: -0.3167\n",
            "Epoch 430, Train Loss: -0.3033\n",
            "Epoch 431, Train Loss: -0.3030\n",
            "Epoch 432, Train Loss: -0.3003\n",
            "Epoch 433, Train Loss: -0.3097\n",
            "Epoch 434, Train Loss: -0.3118\n",
            "Epoch 435, Train Loss: -0.3039\n",
            "Epoch 436, Train Loss: -0.3097\n",
            "Epoch 437, Train Loss: -0.3164\n",
            "Epoch 438, Train Loss: -0.3152\n",
            "Epoch 439, Train Loss: -0.3196\n",
            "Epoch 440, Train Loss: -0.3168\n",
            "Epoch 441, Train Loss: -0.3185\n",
            "Epoch 442, Train Loss: -0.3192\n",
            "Epoch 443, Train Loss: -0.3217\n",
            "Epoch 444, Train Loss: -0.3196\n",
            "Epoch 445, Train Loss: -0.3146\n",
            "Epoch 446, Train Loss: -0.3206\n",
            "Epoch 447, Train Loss: -0.3234\n",
            "Epoch 448, Train Loss: -0.3195\n",
            "Epoch 449, Train Loss: -0.3215\n",
            "Epoch 450, Train Loss: -0.3097\n",
            "Epoch 451, Train Loss: -0.3222\n",
            "Epoch 452, Train Loss: -0.3203\n",
            "Epoch 453, Train Loss: -0.3218\n",
            "Epoch 454, Train Loss: -0.3052\n",
            "Epoch 455, Train Loss: -0.3144\n",
            "Epoch 456, Train Loss: -0.3128\n",
            "Epoch 457, Train Loss: -0.3215\n",
            "Epoch 458, Train Loss: -0.3179\n",
            "Epoch 459, Train Loss: -0.3194\n",
            "Epoch 460, Train Loss: -0.3198\n",
            "Epoch 461, Train Loss: -0.3232\n",
            "Epoch 462, Train Loss: -0.3242\n",
            "Epoch 463, Train Loss: -0.3088\n",
            "Epoch 464, Train Loss: -0.3226\n",
            "Epoch 465, Train Loss: -0.3117\n",
            "Epoch 466, Train Loss: -0.3043\n",
            "Epoch 467, Train Loss: -0.2997\n",
            "Epoch 468, Train Loss: -0.3189\n",
            "Epoch 469, Train Loss: -0.3201\n",
            "Epoch 470, Train Loss: -0.3205\n",
            "Epoch 471, Train Loss: -0.3204\n",
            "Epoch 472, Train Loss: -0.3276\n",
            "Epoch 473, Train Loss: -0.3179\n",
            "Epoch 474, Train Loss: -0.3224\n",
            "Epoch 475, Train Loss: -0.3234\n",
            "Epoch 476, Train Loss: -0.3255\n",
            "Epoch 477, Train Loss: -0.3307\n",
            "Epoch 478, Train Loss: -0.3239\n",
            "Epoch 479, Train Loss: -0.3256\n",
            "Epoch 480, Train Loss: -0.3249\n",
            "Epoch 481, Train Loss: -0.3270\n",
            "Epoch 482, Train Loss: -0.3166\n",
            "Epoch 483, Train Loss: -0.3252\n",
            "Epoch 484, Train Loss: -0.3213\n",
            "Epoch 485, Train Loss: -0.3256\n",
            "Epoch 486, Train Loss: -0.3112\n",
            "Epoch 487, Train Loss: -0.3134\n",
            "Epoch 488, Train Loss: -0.3183\n",
            "Epoch 489, Train Loss: -0.3253\n",
            "Epoch 490, Train Loss: -0.3276\n",
            "Epoch 491, Train Loss: -0.3220\n",
            "Epoch 492, Train Loss: -0.3195\n",
            "Epoch 493, Train Loss: -0.3255\n",
            "Epoch 494, Train Loss: -0.3139\n",
            "Epoch 495, Train Loss: -0.3173\n",
            "Epoch 496, Train Loss: -0.3227\n",
            "Epoch 497, Train Loss: -0.3252\n",
            "Epoch 498, Train Loss: -0.3229\n",
            "Epoch 499, Train Loss: -0.3277\n",
            "Epoch 500, Train Loss: -0.3133\n"
          ]
        }
      ],
      "source": [
        "input_dim = X_train.shape[1]\n",
        "model = MacroPortfolioNet(input_dim=input_dim, hidden_dim=3)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Assume cash has zero return (or plug in a risk-free rate series if you want)\n",
        "risk_free = 0.0\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "model.train()\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "\n",
        "    for batch_X, batch_vol, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        w = model(batch_X, batch_vol)\n",
        "        risky_w = w[:, :5]\n",
        "        cash_w = w[:, 5]\n",
        "\n",
        "        risky_ret = (risky_w * batch_y).sum(dim=1)\n",
        "        port_ret  = risky_ret   # cash ~ 0\n",
        "\n",
        "        # loss = mean_variance_loss(port_ret, risky_w)\n",
        "        loss = sharpe_ratio_loss(port_ret)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {np.mean(epoch_losses):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA6iKc8ftC7b",
        "outputId": "63010bd5-38f0-49a8-afa2-7ed300b770cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Sharpe: 0.032132305\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_returns = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_vol, batch_y in test_loader:\n",
        "        w = model(batch_X, batch_vol)\n",
        "        risky_w = w[:, :5]\n",
        "\n",
        "        port_ret = (risky_w * batch_y).sum(dim=1)\n",
        "        test_returns.append(port_ret.numpy())\n",
        "\n",
        "test_returns = np.concatenate(test_returns)\n",
        "test_sharpe = test_returns.mean() / test_returns.std()\n",
        "\n",
        "print(\"Test Sharpe:\", test_sharpe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL3RIQSuzvkG",
        "outputId": "5c827d40-3797-45d2-d7d9-8aee266336d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.5055487053020962)"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(test_returns>=0)/len(test_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiEsjhyw0KqK",
        "outputId": "c34e31b7-027f-4fb4-a871-377b5d92c1eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float32(0.013753043)"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(test_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "Z414E896xkZZ",
        "outputId": "77e8cb4f-5e2e-4ae5-cf52-9ac342efe997"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_returns' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/ls/t8r2njx51vs74mk0rncxwb7m0000gn/T/ipykernel_72517/1434681635.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Suppose test_returns is a 1D numpy array of daily portfolio returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Compute cumulative PnL (starting at 0 or starting at 1, choose one)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_returns' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suppose test_returns is a 1D numpy array of daily portfolio returns\n",
        "test_returns = np.array(test_returns)\n",
        "\n",
        "# Compute cumulative PnL (starting at 0 or starting at 1, choose one)\n",
        "cumulative_pnl = np.cumprod(1 + test_returns) - 1  # cumulative return\n",
        "# or:\n",
        "# cumulative_pnl = np.cumsum(test_returns)          # raw PnL\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(cumulative_pnl)\n",
        "plt.title(\"Test Set PnL\")\n",
        "plt.xlabel(\"Day\")\n",
        "plt.ylabel(\"PnL (Cumulative Return)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
